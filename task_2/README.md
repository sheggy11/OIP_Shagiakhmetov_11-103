# Задание 2: 
### Методы:

```
parse_web_content(file_path):
```
Этот метод парсит содержимое веб-страницы, сохраненной в файле, используя библиотеку BeautifulSoup. Возвращает текст страницы.

```
extract_tokens(text):
```
Этот метод токенизирует текст страницы, преобразуя его в список слов. Удаляет стоп-слова и неалфавитные символы.

```
lemmatize_words(tokens):
```
Этот метод лемматизирует токены, приводя их к нормальной форме с использованием библиотеки pymorphy2. Результат представлен в виде словаря, где ключи - леммы, а значения - множества токенов.
```
process_web_documents(directory_path):
```
Основной метод, который обрабатывает веб-документы из указанной директории. Парсит содержимое каждого HTML-файла, извлекает токены и леммы, а затем сохраняет результаты в отдельные файлы.

### Порядок действий:
Указали директорию в которой находятся html страницы
Запустили скрипт task_2.py. Он обрабатывает все HTML-файлы в указанной директории, извлекает токены и леммы, а затем сохраняет результаты в файлах tokens_list.txt и lemmatized_tokens_list.txt.

### Deployment Manual:
1. Установка зависимостей:
```
pip install beautifulsoup4
pip install nltk
pip install pymorphy2
```

2. Запуск скрипта:
```
python task_2.py
```
